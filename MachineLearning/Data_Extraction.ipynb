{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install certifi\n",
    "%pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import certifi\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header_data=pd.read_parquet('header_data.parquet',\"pyarrow\")\n",
    "urls=pd.read_parquet('urls.parquet',\"pyarrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls[urls['url']=='disposalsafety.com']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_urls=urls.drop_duplicates('url')\n",
    "distinct_urls.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Extracting company name from website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_company_name(soup):\n",
    "    # 1. Search for the company name in the <header> tag\n",
    "    header_tag = soup.find('header')\n",
    "    if header_tag:\n",
    "        text = header_tag.get_text(strip=True)\n",
    "        if text:\n",
    "            return text.split('\\n')[0]  # Return the first line of text found in the header\n",
    "\n",
    "    # 2. Search for the company name in the <title> tag\n",
    "    title_tag = soup.find('title')\n",
    "    if title_tag and title_tag.get_text(strip=True):\n",
    "        return title_tag.get_text(strip=True)\n",
    "    \n",
    "    # 3. Search for the company name in meta tags with common keywords\n",
    "    for meta_name in ['company', 'business', 'organization', 'name', 'description']:\n",
    "        meta_tag = soup.find('meta', attrs={'name': meta_name})\n",
    "        if meta_tag and meta_tag.get('content'):\n",
    "            return meta_tag.get('content')\n",
    "    \n",
    "    # 4. Search for company name in JavaScript blocks\n",
    "    scripts = soup.find_all('script')\n",
    "    for script in scripts:\n",
    "        if script.string:\n",
    "            script_text = script.string.strip()\n",
    "            if 'name' in script_text:  # Basic check to find name in JS\n",
    "                # Assuming the name is defined in a JS object or variable\n",
    "                # Example: var companyName = \"Example Corp\";\n",
    "                start_idx = script_text.find('name')\n",
    "                if start_idx != -1:\n",
    "                    # Extract the company name using basic string manipulation\n",
    "                    start_quote = script_text.find('\"', start_idx)\n",
    "                    end_quote = script_text.find('\"', start_quote + 1)\n",
    "                    if start_quote != -1 and end_quote != -1:\n",
    "                        return script_text[start_quote + 1:end_quote]\n",
    "    \n",
    "    # 5. Search for the company name in the SITE_FOOTER element as a fallback\n",
    "    footer_tag = soup.find(id='SITE_FOOTER')\n",
    "    if footer_tag:\n",
    "        footer_text = footer_tag.get_text(strip=True)\n",
    "        if footer_text:\n",
    "            return footer_text.split('\\n')[0]\n",
    "\n",
    "    # If no company name is found, return None\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https = 'https://'\n",
    "http = 'http://'\n",
    "\n",
    "urls['company_name'] = None \n",
    "urls['request_failed'] = False\n",
    "\n",
    "for index, row in urls.iterrows():\n",
    "    url = row['url']\n",
    "    full_url = https + url if not url.startswith('http') else url\n",
    "\n",
    "    try:\n",
    "        # Attempt to fetch the website content using HTTPS\n",
    "        response = requests.get(full_url, allow_redirects=True, timeout=5)\n",
    "        response.raise_for_status()\n",
    "        print(f\"Success with HTTPS: {full_url}\")\n",
    "        \n",
    "        # Parse the HTML with BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Extract the company name using the extract_company_name function\n",
    "        company_name = extract_company_name(soup)\n",
    "        urls.at[index, 'company_name'] = company_name\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"HTTPS failed for {full_url}: {e}\")\n",
    "\n",
    "        # Retry with HTTP\n",
    "        full_url = http + url if not url.startswith('https') else url\n",
    "        try:\n",
    "            response = requests.get(full_url, allow_redirects=True, timeout=5)\n",
    "            response.raise_for_status()\n",
    "            print(f\"Success with HTTP: {full_url}\")\n",
    "            \n",
    "            # Parse the HTML with BeautifulSoup\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Extract the company name using the extract_company_name function\n",
    "            company_name = extract_company_name(soup)\n",
    "            urls.at[index, 'company_name'] = company_name\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"HTTP also failed for {full_url}: {e}\")\n",
    "            urls.at[index, 'request_failed'] = True  # Mark as failed if both attempts fail\n",
    "            continue  # Skip to the next URL in the loop\n",
    "    else:\n",
    "        urls.at[index, 'request_failed'] = False  # Mark as successful if one of the requests worked\n",
    "\n",
    "# Check the resulting DataFrame\n",
    "print(urls.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check the resulting DataFrame\n",
    "print(urls.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 Finding NAICS2 code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required\n",
    "header_data['NAICS2']=header_data['NAICS2'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header_data['NAICS2'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is wrong we will use this only till Header extraction ETA is 12 and half hours\n",
    "\n",
    "header_data.head()\n",
    "header_data['url']=header_data.business_name.str.replace(\" \",\"\")\n",
    "header_data['url']=header_data['url'].str.lower()\n",
    "header_data['url']=header_data['url']+'.com'\n",
    "header_data['url']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "naics_dict = dict(zip(header_data['url'],header_data['NAICS2']))\n",
    "urls['NAICS2'] = urls['url'].apply(lambda x: naics_dict.get(x, 'Not Found'))\n",
    "urls.head(24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header_dict=header_data[['business_name','NAICS2']].to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = DictVectorizer()\n",
    "X=vectorizer.fit_transform(header_dict).toarray()\n",
    "y=header_data['NAICS2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_regressor = SVR()\n",
    "svm_regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tesy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
